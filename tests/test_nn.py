# test file generated by chatgpt

import numpy as np

from autodiff_numpy.nn import Linear, ReLU, Sequential
from autodiff_numpy.tensor import Tensor


def test_linear_layer_forward():
    # Input with batch size of 5 and 10 features
    batch_input = Tensor(np.random.randn(5, 10))
    linear_layer = Linear(10, 3)  # 10 in-features, 3 out-features

    output = linear_layer(batch_input)

    assert isinstance(output, Tensor)
    assert output.data.shape == (5, 3), "Output shape should be (batch_size, out_features)"


def test_linear_layer_parameters():
    linear_layer = Linear(10, 3)
    params = linear_layer.parameters()

    assert len(params) == 2, "Linear layer should have 2 parameters (weight and bias)"
    assert params[0].data.shape == (10, 3), "Weight shape is incorrect"
    assert params[1].data.shape == (1, 3), "Bias shape is incorrect"


def test_sequential_model_forward():
    batch_input = Tensor(np.random.randn(10, 5))  # Batch of 10, 5 features
    model = Sequential(Linear(5, 8), ReLU(), Linear(8, 2))
    output = model(batch_input)

    assert output.data.shape == (10, 2), "Final output shape is incorrect"


def test_sequential_model_parameters():
    model = Sequential(
        Linear(5, 8),
        ReLU(),  # ReLU has no parameters
        Linear(8, 2),
    )
    params = model.parameters()

    # 2 params from first Linear, 2 from second
    assert len(params) == 4, "Should collect parameters from all layers"
    assert params[0].data.shape == (5, 8)  # weight_1
    assert params[1].data.shape == (1, 8)  # bias_1
    assert params[2].data.shape == (8, 2)  # weight_2
    assert params[3].data.shape == (1, 2)  # bias_2
    assert params[3].data.shape == (1, 2)  # bias_2


def test_model_zero_grad():
    batch_input = Tensor(np.random.randn(10, 5))
    targets = Tensor(np.random.randn(10, 2))

    model = Sequential(Linear(5, 8), ReLU(), Linear(8, 2))

    # Run a forward and backward pass to populate gradients
    output = model(batch_input)
    loss = ((output - targets) ** 2).sum()  # A simple dummy loss
    loss.backward()

    # Check that some gradients are non-zero
    some_grad_is_nonzero = any(np.any(p.gradient != 0) for p in model.parameters())
    assert some_grad_is_nonzero, "Gradients should be non-zero after backward pass"

    # Zero out the gradients
    model.zero_grad()

    # Check that all gradients are now zero
    all_grads_are_zero = all(np.all(p.gradient == 0) for p in model.parameters())
    assert all_grads_are_zero, "Gradients should be zero after zero_grad()"
