# test file generated by chatgpt

import numpy as np

from autodiff_numpy.tensor import Tensor


def test_add_forward():
    t1 = Tensor([1, 2, 3])
    t2 = Tensor([4, 5, 6])
    result = t1 + t2
    np.testing.assert_allclose(result.data, np.array([5, 7, 9]))


def test_mul_forward():
    t1 = Tensor([1, 2, 3])
    t2 = Tensor([4, 5, 6])
    result = t1 * t2
    np.testing.assert_allclose(result.data, np.array([4, 10, 18]))


def test_matmul_forward():
    t1 = Tensor([[1, 2], [3, 4]])
    t2 = Tensor([[5, 6], [7, 8]])
    result = t1 @ t2
    expected = np.array([[19, 22], [43, 50]])
    np.testing.assert_allclose(result.data, expected)


def test_relu_forward():
    t = Tensor([-1, 0, 5, -10])
    result = t.relu()
    np.testing.assert_allclose(result.data, np.array([0, 0, 5, 0]))


def test_add_backward():
    t1 = Tensor([1, 2, 3], label="t1")
    t2 = Tensor([4, 5, 6], label="t2")
    result = t1 + t2
    result.gradient = np.array([1, 1, 1])
    result._backward()
    np.testing.assert_allclose(t1.gradient, np.array([1, 1, 1]))
    np.testing.assert_allclose(t2.gradient, np.array([1, 1, 1]))


def test_mul_backward():
    t1 = Tensor([1.0, 2.0, 3.0])
    t2 = Tensor([4.0, 5.0, 6.0])
    result = t1 * t2
    # To start backprop, we need a scalar loss. Let's sum the result.
    loss = Tensor(np.sum(result.data))
    loss.gradient = np.ones_like(loss.data)  # Start with gradient of 1

    # Manually set the gradient for `result` as if it came from the sum
    result.gradient = np.ones_like(result.data)
    result._backward()

    # d(loss)/dt1 = t2.data
    # d(loss)/dt2 = t1.data
    np.testing.assert_allclose(t1.gradient, t2.data)
    np.testing.assert_allclose(t2.gradient, t1.data)


def test_matmul_backward():
    t1 = Tensor([[1.0, 2.0], [3.0, 4.0]])
    t2 = Tensor([[5.0, 6.0], [7.0, 8.0]])
    result = t1 @ t2

    # Sum the result to get a scalar loss for backward pass
    loss = result.sum()
    loss.backward()

    # Gradients for matmul A @ B are:
    # grad_A = grad_out @ B.T
    # grad_B = A.T @ grad_out
    # Here, grad_out is a matrix of ones with the same shape as `result`
    grad_out = np.ones_like(result.data)
    expected_grad_t1 = grad_out @ t2.data.T
    expected_grad_t2 = t1.data.T @ grad_out

    np.testing.assert_allclose(t1.gradient, expected_grad_t1)
    np.testing.assert_allclose(t2.gradient, expected_grad_t2)


def test_complex_graph_backward():
    """Tests a more complex graph with chain rule."""
    a = Tensor(2.0, label="a")
    b = Tensor(3.0, label="b")
    c = a * b  # c = 6
    d = Tensor(4.0, label="d")
    e = c + d  # e = 10
    f = e.relu()  # f = 10

    # Start backpropagation from the end
    f.backward()

    # Manually compute gradients
    # df/de = 1 (since e > 0)
    # de/dc = 1
    # de/dd = 1
    # dc/da = b = 3
    # dc/db = a = 2
    #
    # Using chain rule:
    # df/da = (df/de) * (de/dc) * (dc/da) = 1 * 1 * 3 = 3
    # df/db = (df/de) * (de/dc) * (dc/db) = 1 * 1 * 2 = 2
    # df/dd = (df/de) * (de/dd) = 1 * 1 = 1

    assert a.gradient == 3.0
    assert b.gradient == 2.0
    assert d.gradient == 1.0
    assert c.gradient == 1.0
