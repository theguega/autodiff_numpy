# test file generated by gemini 2.5 pro

from autodiff_numpy.tensor import Tensor


def test_week4_tutorial():
    """
    Replicates the manual backpropagation exercise from the PDF to verify
    the correctness of the Tensor library's gradient calculations and update rule,
    using standard assert statements.
    """
    # Define a small tolerance for floating-point comparisons
    tolerance = 1e-6

    # --------------------------------------------------------------------
    # 1. ARRANGE: Set up the network parameters from the PDF
    # --------------------------------------------------------------------
    w1_1 = Tensor(1.0, label="w1_1")
    w2_1 = Tensor(0.8, label="w2_1")
    w2_2 = Tensor(-0.6, label="w2_2")
    w3_1 = Tensor(0.5, label="w3_1")
    w3_2 = Tensor(0.5, label="w3_2")
    b2 = Tensor(-0.5, label="b2")
    x = Tensor(-1.0, label="x")
    y = Tensor(0.5, label="y")
    learning_rate = 0.1

    # --------------------------------------------------------------------
    # 2. ACT: Perform the forward pass, exactly as in the PDF
    # --------------------------------------------------------------------
    h1 = x * w1_1
    h2_1 = (h1 * w2_1 + b2).relu()
    h2_2 = (h1 * w2_2 + b2).relu()
    y_hat = h2_1 * w3_1 + h2_2 * w3_2
    loss = ((y_hat - y) ** 2) * 0.5

    # --------------------------------------------------------------------
    # 3. ASSERT: Check that forward pass values match the PDF
    # --------------------------------------------------------------------
    assert abs(h1.data - (-1.0)) < tolerance
    assert h2_1.data == 0  # This is an exact integer
    assert abs(h2_2.data - 0.1) < tolerance
    assert abs(y_hat.data - 0.05) < tolerance
    assert abs(loss.data - 0.10125) < tolerance

    # --------------------------------------------------------------------
    # 4. ACT: Perform the backward pass
    # --------------------------------------------------------------------
    loss.backward()

    # --------------------------------------------------------------------
    # 5. ASSERT: Check that calculated gradients match the PDF
    # --------------------------------------------------------------------
    # These are the ∂L/∂w values from the "Backward propagation" section
    assert w3_1.gradient == 0
    assert abs(w3_2.gradient - (-0.045)) < tolerance
    assert w2_1.gradient == 0
    assert abs(w2_2.gradient - 0.225) < tolerance
    assert abs(w1_1.gradient - (-0.135)) < tolerance

    # --------------------------------------------------------------------
    # 6. ACT: Perform the weight update step
    # --------------------------------------------------------------------
    new_w1_1 = w1_1.data - learning_rate * w1_1.gradient
    # new_w2_1 = w2_1.data - learning_rate * w2_1.gradient
    new_w2_2 = w2_2.data - learning_rate * w2_2.gradient
    # new_w3_1 = w3_1.data - learning_rate * w3_1.gradient
    new_w3_2 = w3_2.data - learning_rate * w3_2.gradient

    # --------------------------------------------------------------------
    # 7. ASSERT: Check that the new weights match the PDF
    # --------------------------------------------------------------------
    # These are the values from the "Weight update" section
    assert abs(new_w3_2 - 0.5045) < tolerance
    assert abs(new_w2_2 - (-0.6225)) < tolerance
    assert abs(new_w1_1 - 1.0135) < tolerance
